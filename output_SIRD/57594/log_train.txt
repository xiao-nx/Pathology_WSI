Equation name for problem: SIRD

Network model of dealing with SIR: DNN_FourierBase

Network model of dealing with parameters: DNN_FourierBase

The input activate function for SIRD: [sin;cos]

The hidden-layer activate function for SIRD: tanh

The input activate function for parameter: [sin;cos]

The hidden-layer activate function for parameter: tanh

hidden layers for SIR: (35, 50, 30, 30, 20)

hidden layers for parameters: (35, 50, 30, 30, 20)

The scale for frequency to SIR NN: [ 1  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]

Repeat the high-frequency scale or not for SIR-NN: False

The scale for frequency to SIR NN: [ 1  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]

Repeat the high-frequency scale or not for para-NN: False

Init learning rate: 0.002

Decay to learning rate: 0.0001

The type for Loss function: L2_loss

optimizer:Adam

no activate the stop_step and given_step = default: 200000

Initial penalty for difference of predict and true: 1

The model of regular weights and biases: L2

Regularization parameter for weights and biases: 5e-05

Size 2 training set: 70

Batch-size 2 training: 20

Batch-size 2 testing: 10

The test data about i:
[[0.00028203 0.00030609 0.00032435 0.00033565 0.00035275 0.00038551
  0.00041507 0.00044464 0.00047449 0.00050203 0.00052348 0.00054029
  0.00058319 0.00062754 0.00067275 0.00072058 0.00077333 0.00080812
  0.00084464 0.00092464 0.00100058 0.00108928 0.00123768 0.00138783
  0.00148609 0.00159768 0.00179333 0.00196754 0.00216464 0.00235246
  0.00253217 0.00262928 0.00272783 0.00296667 0.00315246 0.00333623
  0.00357333 0.00377971 0.00386116 0.00392812 0.00416783 0.00436029
  0.00457159 0.00480145 0.00503275 0.00513188 0.00522203 0.00550348
  0.0057513  0.00597362 0.00625565 0.00648522 0.00659855 0.00670174
  0.00681855 0.0070458  0.0072429  0.00740841 0.00754    0.0075887
  0.00762957 0.00779623 0.00793507 0.00808377 0.0082     0.00831536
  0.00836319 0.00840232 0.00853217 0.00865681 0.00875333 0.00881681
  0.00882986 0.00883043 0.00883217 0.00889652 0.0089513  0.0090713
  0.00918116 0.00928435 0.00941072 0.00954203 0.00963101 0.00970116
  0.00978638 0.00989072 0.01003362 0.01015449 0.01030406 0.01039449
  0.01052261 0.01064232 0.01078551 0.01090551 0.01090551 0.01105391
  0.01117942 0.0113429  0.01147507 0.01164145 0.01181652 0.01204957
  0.01225536 0.01239768 0.01251304 0.01267884 0.0128542  0.01304725
  0.01317971 0.01339246 0.0136542  0.01375565 0.01390174 0.01412203
  0.01434435 0.0145771  0.01482696 0.01501536 0.01515391 0.01534696
  0.0155629  0.01578638 0.01599652 0.01621652 0.0163942  0.0165687
  0.01674754 0.0169971  0.01715507 0.01742058 0.01765159 0.01783072
  0.01792435 0.01805884 0.01825884 0.01847043 0.01867043 0.01888464
  0.01904812 0.01914812 0.01930957 0.01950957 0.0197487  0.01996145
  0.02016927 0.02037623 0.02049478 0.02064812 0.02098261 0.02122899
  0.02152377 0.02179391 0.02198957 0.02213188 0.02234348 0.02264435
  0.0228887  0.02315362 0.02335855 0.02354348 0.02365449 0.02372986
  0.02384029 0.02397565 0.02422841 0.02443797 0.0246229  0.02473942
  0.02487333 0.02513681 0.0254513  0.02571623 0.02609188 0.02636
  0.02649913 0.02669565 0.02696    0.02730116 0.02772725 0.02803884
  0.02830087 0.02853536 0.02873449 0.02904348 0.02938145 0.02979333
  0.03009449 0.03037652 0.03064927 0.03091333 0.03128174 0.03168464
  0.03212406 0.03254145 0.03288087 0.03320986 0.03360667 0.03394377
  0.03460754 0.03509855 0.03559768 0.03606927 0.0363858  0.03669304
  0.03714551 0.03764145 0.03829623 0.03878319 0.03923826 0.03986551
  0.04041855 0.04124957 0.04216377 0.04303536 0.04367304 0.04452753
  0.04553507 0.04664435 0.04778695 0.04936435 0.0507113  0.05242377
  0.05356174 0.05498    0.0563971  0.0584913  0.06009826 0.06261681
  0.06480609 0.06696174 0.06868087 0.07015739 0.07243652 0.0744058
  0.07621797 0.07830638 0.08014493 0.08200464 0.08385594 0.08385594
  0.08550753 0.08812261 0.09071565 0.09239507 0.09342377 0.09492087
  0.09670319 0.09825304 0.10008145 0.10169913 0.10323247 0.10411681
  0.1054258  0.10644    0.10752696 0.10881101 0.10980377 0.11067855
  0.11135188 0.11200348 0.11280319 0.11359102 0.11438695 0.11516493
  0.11574232 0.11623507 0.11667217 0.11721826 0.11721826 0.11783913
  0.11856841 0.11888058 0.11916232 0.11974116 0.12037739 0.12037739
  0.12111072 0.12189681 0.12280811 0.12326406 0.12393826 0.12451304
  0.12520117 0.12591681 0.12654261 0.12682667 0.12720783 0.12763885
  0.1280971  0.1285687  0.12899913 0.1293855  0.12966637 0.12993275
  0.13028754 0.13065565 0.13109218 0.13153856 0.13188086 0.13211101
  0.13231595 0.13255565 0.1329371  0.13326    0.13357073 0.1338571 ]]

The test data about s:
[[0.9996232  0.99958867 0.9995606  0.9995368  0.99951655 0.9994783
  0.99942577 0.9993722  0.99932986 0.9992858  0.99924666 0.99921566
  0.99917275 0.99910897 0.9990548  0.99898374 0.9989177  0.9988678
  0.9988191  0.99872696 0.99863595 0.99852896 0.9983171  0.99815017
  0.9980345  0.9978881  0.99767274 0.99747825 0.99724317 0.997018
  0.9968064  0.99667597 0.99639827 0.9958125  0.99551016 0.99521303
  0.9948881  0.994569   0.9943064  0.9940771  0.9936478  0.9932562
  0.9928815  0.99255043 0.99221015 0.9918884  0.9916168  0.991162
  0.99068433 0.9902545  0.98988754 0.98958814 0.9892432  0.98895246
  0.988629   0.9881916  0.98775536 0.9874809  0.9872612  0.98693305
  0.98664725 0.9862614  0.9858438  0.9854623  0.98524463 0.9850113
  0.9848443  0.98458636 0.98426056 0.98396665 0.9837272  0.9836035
  0.9835374  0.9833626  0.9832203  0.9830038  0.9828313  0.98259217
  0.98242986 0.9822803  0.98200697 0.9817406  0.9815339  0.98136437
  0.9811878  0.9810383  0.98084927 0.98061275 0.9803426  0.9801287
  0.97988987 0.97966814 0.9794586  0.9792829  0.9792829  0.9788255
  0.9785545  0.9782539  0.97796696 0.9777148  0.9774661  0.9770667
  0.97669476 0.976373   0.9760965  0.9758032  0.9755933  0.97531766
  0.9749687  0.9745548  0.9740771  0.9737519  0.97339505 0.97308815
  0.97276056 0.972311   0.9718267  0.97141534 0.971047   0.9706632
  0.97035044 0.9700313  0.96957886 0.96914554 0.9687591  0.9683339
  0.96792114 0.96755916 0.9673026  0.9667948  0.9663293  0.96591854
  0.96559507 0.96525156 0.9649072  0.9646026  0.96416956 0.96373916
  0.96338177 0.9630736  0.9627119  0.9624046  0.9620719  0.9616313
  0.9612261  0.9608191  0.96050525 0.9601588  0.9596751  0.95933247
  0.9587748  0.95826954 0.95785916 0.95747393 0.95696    0.9564675
  0.95611537 0.95556813 0.9550843  0.9546261  0.95427334 0.9539583
  0.95373625 0.95350087 0.95298666 0.95253825 0.9521272  0.951802
  0.9514838  0.9511322  0.95071596 0.9503933  0.9497478  0.9492357
  0.9489026  0.948509   0.94814086 0.94768375 0.9469641  0.94636345
  0.94578695 0.94524664 0.9447493  0.9442661  0.9437768  0.94296896
  0.9422878  0.9416374  0.94101626 0.9404577  0.9399026  0.93936205
  0.9385316  0.93778753 0.93709797 0.93640524 0.9356658  0.93511534
  0.93428177 0.93330985 0.93235886 0.9314084  0.93060553 0.9298464
  0.929151   0.92844754 0.9272049  0.92619246 0.9251751  0.9240568
  0.9230841  0.92199737 0.9208829  0.91933596 0.9180261  0.9164554
  0.9148342  0.91306955 0.91152346 0.9095397  0.9071365  0.90439737
  0.90225244 0.8997896  0.8972499  0.89447683 0.892193   0.88807654
  0.8842438  0.8801307  0.87635595 0.8727762  0.86917305 0.86600524
  0.8615461  0.8570672  0.85302144 0.8492562  0.84539187 0.84539187
  0.84247565 0.8362235  0.831371   0.82754666 0.8245023  0.82104695
  0.8181365  0.81566113 0.8115125  0.80780756 0.8045522  0.80342406
  0.800562   0.79834205 0.79629886 0.7927493  0.7899139  0.7874261
  0.7853719  0.78337073 0.7818852  0.78044057 0.77825075 0.7762084
  0.77464235 0.7732577  0.7718835  0.77080524 0.77080524 0.7697516
  0.7672096  0.76621854 0.7652719  0.76404405 0.76298404 0.76298404
  0.76191074 0.7595962  0.75806177 0.75727856 0.7565145  0.7554632
  0.7544084  0.75286317 0.75158405 0.75064695 0.7497968  0.74919796
  0.7482113  0.74733305 0.7460328  0.74499273 0.7441701  0.7433887
  0.7425481  0.7418855  0.741213   0.7401696  0.7394461  0.738793
  0.7382052  0.7376287  0.7369933  0.736473   0.7357136  0.7350307 ]]

train epoch: 0,time: 3.252
learning rate: 0.002000
penalty for difference of predict and true : 1.000000
penalty weights and biases for S: 0.009377
penalty weights and biases for I: 0.009306
penalty weights and biases for params: 0.0093331449
penalty weights and biases for D: 3167.4726562500
loss for S: 2789.3361816406250000
loss for I: 1931.9692382812500000
loss for params: 1950.5883789062500000
loss for D: 9839.3935546875000000
train epoch: 1000,time: 6.824
learning rate: 0.001809
penalty for difference of predict and true : 1.000000
penalty weights and biases for S: 0.009193
penalty weights and biases for I: 0.009077
penalty weights and biases for params: 0.0092590312
penalty weights and biases for D: 75.0235671997
loss for S: 4.3815116882324219
loss for I: 15.1635522842407227
loss for params: 0.1082818135619164
loss for D: 94.6947326660156250
train epoch: 2000,time: 10.531
learning rate: 0.001637
penalty for difference of predict and true : 1.000000
penalty weights and biases for S: 0.009415
penalty weights and biases for I: 0.008917
penalty weights and biases for params: 0.0093393922
penalty weights and biases for D: 40.2860679626
loss for S: 1.1236151456832886
loss for I: 6.8663907051086426
loss for params: 0.0559948310256004
loss for D: 48.3419227600097656
train epoch: 3000,time: 15.429
learning rate: 0.001481
penalty for difference of predict and true : 1.000000
penalty weights and biases for S: 0.009688
penalty weights and biases for I: 0.008836
penalty weights and biases for params: 0.0093698632
penalty weights and biases for D: 6.0074486732
loss for S: 1.3708746433258057
loss for I: 2.6831810474395752
loss for params: 0.0248814001679420
loss for D: 10.0919504165649414
train epoch: 4000,time: 19.796
learning rate: 0.001340
penalty for difference of predict and true : 1.000000
penalty weights and biases for S: 0.009547
penalty weights and biases for I: 0.008740
penalty weights and biases for params: 0.0092702508
penalty weights and biases for D: 3.2779114246
loss for S: 0.2699166536331177
loss for I: 2.1024131774902344
loss for params: 0.0167321208864450
loss for D: 5.6702523231506348
